{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78c6add",
   "metadata": {},
   "source": [
    "Coleta de Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf427f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fontes de Notícias\n",
    "\n",
    "fonte_g1 = \"https://g1.globo.com/economia/dolar/\"\n",
    "fonte_cnn = \"https://www.cnnbrasil.com.br/tudo-sobre/dolar/\"\n",
    "fonte_folha = \"https://www1.folha.uol.com.br/folha-topicos/dolar/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccf2f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importando Bibliotecas para Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf38f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraindo Notícias do G1 com o Selenium\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(fonte_g1)\n",
    "time.sleep(5)  # Aguarda o carregamento da página\n",
    "driver.find_element(By.CSS_SELECTOR, \"svg.fc-cancel-icon-svg\").click() # Fecha o pop-up de ADS\n",
    "\n",
    "# Scroll para carregar mais notícias\n",
    "for _ in range(10):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "html_g1 = driver.page_source\n",
    "\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07bbe3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(html_g1, \"html.parser\")\n",
    "\n",
    "# Tratando os dados do G1\n",
    "url_imagem = [img[\"src\"] for img in soup.select(\"img.bstn-fd-picture-image\")]\n",
    "titulos = [t.get_text(strip=True) for t in soup.select(\"div.feed-post-body-title a\")]\n",
    "links = [a[\"href\"] for a in soup.select(\"a.feed-post-link\")]\n",
    "data_publicacao = [\n",
    "    f\"{link.split('/')[7]}/{link.split('/')[6]}/{link.split('/')[5]}\"\n",
    "    for link in links\n",
    "]\n",
    "\n",
    "# Salvando os dados em um DataFrame com a data de extração\n",
    "data_extracao = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "df_g1 = pd.DataFrame({\n",
    "    \"urlImagem\": url_imagem,\n",
    "    \"dataPublicacao\": data_publicacao,\n",
    "    \"titulo\": titulos,\n",
    "    \"link\": links,\n",
    "    \"fonte\": \"G1\",\n",
    "    \"dataExtracao\": data_extracao\n",
    "})\n",
    "\n",
    "df_noticias = df_g1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf3084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraindo Notícias do CNN \n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(fonte_cnn)\n",
    "time.sleep(5)  # Aguarda o carregamento da página\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "botao_ok = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".btn-agree button\")))\n",
    "\n",
    "# Clica no botão\n",
    "botao_ok.click()\n",
    "\n",
    "\n",
    "for i in range(2):  # Navega por 2 páginas\n",
    "    time.sleep(5)\n",
    "    html_cnn = driver.page_source\n",
    "    soup = BeautifulSoup(html_cnn, \"html.parser\")\n",
    "    container = soup.select_one('ul[data-section=\"article_list\"]')\n",
    "    \n",
    "\n",
    "    # Printa a pagina percorrida\n",
    "    url_imagem= []\n",
    "    if container:\n",
    "        url_imagem = [img[\"src\"] for img in container.select(\"img\") if img.get(\"src\")]\n",
    "    titulos = [t.get_text(strip=True) for t in soup.select(\"div.flex.flex-col.gap-4 h2\")]\n",
    "    links = [a[\"href\"] for a in soup.select(\"div.flex.flex-col.gap-4 a\")]\n",
    "    links = [item for item in links if item != 'https://www.cnnbrasil.com.br/ao-vivo/']\n",
    "    data_publicacao = []\n",
    "    for tag in soup.select(\"time\"):\n",
    "        raw = tag.get(\"datetime\")\n",
    "        if raw:\n",
    "            data_publicacao.append(datetime.fromisoformat(raw).strftime(\"%d/%m/%Y\"))\n",
    "\n",
    "    data_extracao = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    df_cnn = pd.DataFrame({\n",
    "        \"urlImagem\": url_imagem,\n",
    "        \"dataPublicacao\": data_publicacao,\n",
    "        \"titulo\": titulos,\n",
    "        \"link\": links,\n",
    "        \"fonte\": \"CNN\",\n",
    "        \"dataExtracao\": data_extracao\n",
    "    })\n",
    "\n",
    "    df_noticias = pd.concat([df_noticias, df_cnn], ignore_index=True)\n",
    "\n",
    "    driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Ir para próxima página']\").click()    \n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(fonte_folha)\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "for i in range(4):\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"main .row, main article, main li\")))\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        found = False\n",
    "\n",
    "        for _ in range(50):  \n",
    "            candidatos = driver.find_elements(By.CSS_SELECTOR, \"button.c-button.c-button--expand\")\n",
    "        \n",
    "            if candidatos:\n",
    "                botao = candidatos[0]\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", botao)\n",
    "                time.sleep(0.8)\n",
    "                try:\n",
    "                    wait.until(EC.element_to_be_clickable((By.XPATH, \".//button[contains(@class,'c-button') or @data-pagination-trigger]\")))\n",
    "                except TimeoutException:\n",
    "                    pass \n",
    "\n",
    "                try:\n",
    "                    botao.click()\n",
    "                except Exception:\n",
    "                    driver.execute_script(\"arguments[0].click();\", botao)\n",
    "                break\n",
    "    finally:\n",
    "        time.sleep(2)\n",
    "\n",
    "html_folha = driver.page_source       \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_folha, \"html.parser\") \n",
    "\n",
    "url_imagem = [img.get(\"data-src\") for img in soup.select(\"img.c-headline__image\") if img.get(\"data-src\")]\n",
    "titulos = [t.get_text(strip=True) for t in soup.select(\"h2.c-headline__title\")]  \n",
    "data_publicacao = [t.get(\"datetime\") for t in soup.select(\"time.c-headline__dateline[itemprop='datePublished']\")]\n",
    "links = [a[\"href\"] for a in soup.select(\"a.c-headline__url\")]\n",
    "\n",
    "# Converter a dataPublicacao para dd/mm/yyyy\n",
    "data_publicacao = [\n",
    "    datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\").strftime(\"%d/%m/%Y\")\n",
    "    for d in data_publicacao if d\n",
    "]\n",
    "\n",
    "def pad(lst, n, fill=\"\"):\n",
    "    return lst + [fill] * (n - len(lst))\n",
    "\n",
    "n = max(len(url_imagem), len(data_publicacao), len(titulos), len(links))\n",
    "\n",
    "df_folha = pd.DataFrame({\n",
    "    \"urlImagem\":     pad(url_imagem, n, \"\"),\n",
    "    \"dataPublicacao\":pad(data_publicacao, n, \"\"),\n",
    "    \"titulo\":        pad(titulos, n, \"\"),\n",
    "    \"link\":          pad(links, n, \"\"),\n",
    "    \"fonte\":         [\"Folha\"] * n,\n",
    "    \"dataExtracao\":  [datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")] * n,\n",
    "})\n",
    "\n",
    "df_folha = df_folha[df_folha['urlImagem'] != '']\n",
    "\n",
    "df_noticias = pd.concat([df_noticias, df_cnn], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noticias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
