{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474c57db",
   "metadata": {},
   "source": [
    "# üì∞ ETL de Not√≠cias ‚Äî Projeto Web Unifor ETL\n",
    "\n",
    "## üéØ Objetivo\n",
    "Este notebook implementa a etapa de **extra√ß√£o, transforma√ß√£o e carga (ETL)** de not√≠cias relacionadas ao **d√≥lar americano** em diferentes portais de m√≠dia.  \n",
    "O objetivo √© construir uma base anal√≠tica consolidada em **DuckDB**, contendo not√≠cias coletadas de m√∫ltiplas fontes jornal√≠sticas para posterior an√°lise de tend√™ncias, correla√ß√£o com indicadores econ√¥micos e varia√ß√£o cambial.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê Fontes de Dados\n",
    "As not√≠cias s√£o coletadas diretamente de portais jornal√≠sticos oficiais, utilizando t√©cnicas de **Web Scraping com Selenium e BeautifulSoup**:\n",
    "\n",
    "| Fonte | URL |\n",
    "|--------|-----|\n",
    "| **G1** | [https://g1.globo.com/economia/dolar/](https://g1.globo.com/economia/dolar/) |\n",
    "| **CNN Brasil** | [https://www.cnnbrasil.com.br/tudo-sobre/dolar/](https://www.cnnbrasil.com.br/tudo-sobre/dolar/) |\n",
    "| **Folha de S. Paulo** | [https://www1.folha.uol.com.br/folha-topicos/dolar/](https://www1.folha.uol.com.br/folha-topicos/dolar/) |\n",
    "\n",
    "Cada fonte √© processada de forma independente, com adapta√ß√£o do seletor CSS conforme a estrutura HTML de cada portal.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Estrutura do Pipeline\n",
    "\n",
    "1. **Coleta:**  \n",
    "   - Navega√ß√£o automatizada com Selenium (rolagem e m√∫ltiplas p√°ginas).  \n",
    "   - Captura de t√≠tulos, datas, links e imagens.  \n",
    "   - Convers√£o de datas e padroniza√ß√£o de campos.\n",
    "\n",
    "2. **Transforma√ß√£o:**  \n",
    "   - Normaliza√ß√£o de colunas (`dataPublicacao`, `dataExtracao`).  \n",
    "   - Preenchimento de listas com tamanhos diferentes (`pad`).  \n",
    "   - Remo√ß√£o de duplicatas via hash composto (`titulo` + `fonte`).\n",
    "\n",
    "3. **Carga:**  \n",
    "   - Salvamento dos dados no banco anal√≠tico **DuckDB (`dados_dolar.duckdb`)**.  \n",
    "   - Cria√ß√£o da tabela `noticias` com schema padronizado.  \n",
    "   - Exporta√ß√£o adicional em `.csv` para interoperabilidade.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Estrutura da Tabela `noticias`\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o |\n",
    "|--------|------|-----------|\n",
    "| `urlImagem` | TEXT | URL da imagem destacada da not√≠cia |\n",
    "| `dataPublicacao` | TIMESTAMP | Data de publica√ß√£o extra√≠da da p√°gina |\n",
    "| `titulo` | TEXT | T√≠tulo da not√≠cia |\n",
    "| `link` | TEXT | URL da not√≠cia |\n",
    "| `fonte` | TEXT | Nome do portal de origem |\n",
    "| `dataExtracao` | TIMESTAMP | Data e hora da extra√ß√£o do dado |\n",
    "| `hash` | TEXT | Identificador √∫nico (hash de `titulo` + `fonte`) |\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Logs e Monitoramento\n",
    "Cada etapa do notebook possui **logs estruturados** com:\n",
    "- Status de execu√ß√£o (INFO, WARNING, ERROR).  \n",
    "- Tempo total de cada etapa (G1, CNN, Folha, ETL e DB).  \n",
    "- Quantidade de registros coletados por fonte.  \n",
    "- Registro autom√°tico em `logs/noticias.log`.\n",
    "\n",
    "Os logs permitem reexecutar e diagnosticar o ETL de forma audit√°vel e transparente.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Reprodutibilidade\n",
    "- Ambiente configurado com bibliotecas listadas em `requirements.txt`.  \n",
    "- Notebook execut√°vel de ponta a ponta, sem depend√™ncias externas.  \n",
    "- Sa√≠das persistentes no arquivo `dados_dolar.duckdb` e export em `exports/noticias.csv`.  \n",
    "- Suporte a reexecu√ß√£o automatizada (opcional) via `papermill` ou `nbconvert`.\n",
    "\n",
    "---\n",
    "\n",
    "üìÖ **√öltima atualiza√ß√£o:** 25/10/2025\n",
    "üë®‚Äçüíª **Autor:** ANDERSON DE OLIVEIRA SILVA ‚Äî Projeto Web Unifor ETL (2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf427f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 14:18:06 | INFO | noticias | ===== IN√çCIO EXECU√á√ÉO noticias.ipynb =====\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOGGING SETUP\n",
    "# =========================\n",
    "import os\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def get_logger(name=\"noticias\", level=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if logger.handlers:\n",
    "        return logger  # evita m√∫ltiplos handlers no notebook\n",
    "\n",
    "    # Formato com campos extras opcionais (fonte, etapa)\n",
    "    fmt = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
    "    datefmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    file_handler = RotatingFileHandler(\n",
    "        os.path.join(LOG_DIR, \"noticias.log\"),\n",
    "        maxBytes=2_000_000,\n",
    "        backupCount=5,\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    file_handler.setLevel(level)\n",
    "    file_handler.setFormatter(logging.Formatter(fmt, datefmt=datefmt))\n",
    "\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(level)\n",
    "    console.setFormatter(logging.Formatter(fmt, datefmt=datefmt))\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console)\n",
    "    return logger\n",
    "\n",
    "log = get_logger()\n",
    "log.info(\"===== IN√çCIO EXECU√á√ÉO noticias.ipynb =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2615d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMPORTS & FONTES\n",
    "# =========================\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium import webdriver\n",
    "\n",
    "# Fontes de Not√≠cias\n",
    "fonte_g1   = \"https://g1.globo.com/economia/dolar/\"\n",
    "fonte_cnn  = \"https://www.cnnbrasil.com.br/tudo-sobre/dolar/\"\n",
    "fonte_folha= \"https://www1.folha.uol.com.br/folha-topicos/dolar/\"\n",
    "\n",
    "# Utils\n",
    "def agora():\n",
    "    return datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "def pad(lst, n, fill=\"\"):\n",
    "    return lst + [fill] * max(0, n - len(lst))\n",
    "\n",
    "def safe_df_from_lists(fonte, url_imagem, data_publicacao, titulos, links):\n",
    "    \"\"\"\n",
    "    Garante que todas as colunas tenham o mesmo tamanho (com pad) e loga diferen√ßas.\n",
    "    \"\"\"\n",
    "    n_max = max(len(url_imagem), len(data_publicacao), len(titulos), len(links))\n",
    "    if len({len(url_imagem), len(data_publicacao), len(titulos), len(links)}) != 1:\n",
    "        log.warning(f\"[{fonte}] Listas com tamanhos diferentes: \"\n",
    "                    f\"img={len(url_imagem)} | data={len(data_publicacao)} | tit={len(titulos)} | link={len(links)}. Fazendo pad para {n_max}\")\n",
    "    df = pd.DataFrame({\n",
    "        \"urlImagem\":      pad(url_imagem, n_max, \"\"),\n",
    "        \"dataPublicacao\": pad(data_publicacao, n_max, \"\"),\n",
    "        \"titulo\":         pad(titulos, n_max, \"\"),\n",
    "        \"link\":           pad(links, n_max, \"\"),\n",
    "        \"fonte\":          [fonte]*n_max,\n",
    "        \"dataExtracao\":   [agora()]*n_max,\n",
    "    })\n",
    "    # filtra linhas totalmente vazias\n",
    "    df = df[~(df[\"titulo\"].eq(\"\") & df[\"link\"].eq(\"\") & df[\"urlImagem\"].eq(\"\"))]\n",
    "    return df\n",
    "\n",
    "def nova_sessao_chrome():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # exemplo: options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91800d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 14:18:30 | INFO | noticias | [G1] Iniciando extra√ß√£o\n",
      "2025-10-25 14:18:54 | INFO | noticias | [G1] Pop-up fechado\n",
      "2025-10-25 14:19:16 | INFO | noticias | [G1] Coletados 32 registros\n",
      "2025-10-25 14:19:18 | INFO | noticias | [G1] Driver encerrado\n",
      "2025-10-25 14:19:18 | INFO | noticias | [G1] Conclu√≠do em 47.86s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# G1\n",
    "# =========================\n",
    "inicio = time.perf_counter()\n",
    "log.info(\"[G1] Iniciando extra√ß√£o\")\n",
    "\n",
    "driver = None\n",
    "df_noticias = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    driver = nova_sessao_chrome()\n",
    "    driver.get(fonte_g1)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        driver.find_element(By.CSS_SELECTOR, \"svg.fc-cancel-icon-svg\").click()\n",
    "        log.info(\"[G1] Pop-up fechado\")\n",
    "    except Exception:\n",
    "        log.info(\"[G1] Pop-up n√£o encontrado (ok)\")\n",
    "\n",
    "    # Scroll p/ carregar mais\n",
    "    for _ in range(10):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    html_g1 = driver.page_source\n",
    "    soup = BeautifulSoup(html_g1, \"html.parser\")\n",
    "\n",
    "    url_imagem = [img.get(\"src\",\"\") for img in soup.select(\"img.bstn-fd-picture-image\")]\n",
    "    titulos    = [t.get_text(strip=True) for t in soup.select(\"div.feed-post-body-title a\")]\n",
    "    links      = [a.get(\"href\",\"\") for a in soup.select(\"a.feed-post-link\")]\n",
    "\n",
    "    # Deriva data da URL (se falhar, deixa vazio)\n",
    "    data_publicacao = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            partes = link.split(\"/\")\n",
    "            data_publicacao.append(f\"{partes[7]}/{partes[6]}/{partes[5]}\")\n",
    "        except Exception:\n",
    "            data_publicacao.append(\"\")\n",
    "\n",
    "    df_g1 = safe_df_from_lists(\"G1\", url_imagem, data_publicacao, titulos, links)\n",
    "    df_noticias = df_g1.copy()\n",
    "\n",
    "    log.info(f\"[G1] Coletados {len(df_g1)} registros\")\n",
    "except Exception as e:\n",
    "    log.exception(f\"[G1] ERRO na extra√ß√£o: {e}\")\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        log.info(\"[G1] Driver encerrado\")\n",
    "\n",
    "log.info(f\"[G1] Conclu√≠do em {time.perf_counter()-inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9872d120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 14:19:44 | INFO | noticias | [CNN] Iniciando extra√ß√£o\n",
      "2025-10-25 14:19:55 | INFO | noticias | [CNN] Aceite de cookies clicado\n",
      "2025-10-25 14:19:58 | INFO | noticias | [CNN] P√°gina 1: 10 registros\n",
      "2025-10-25 14:20:01 | INFO | noticias | [CNN] Pr√≥xima p√°gina clicada\n",
      "2025-10-25 14:20:04 | INFO | noticias | [CNN] P√°gina 2: 10 registros\n",
      "2025-10-25 14:20:06 | INFO | noticias | [CNN] Pr√≥xima p√°gina clicada\n",
      "2025-10-25 14:20:06 | INFO | noticias | [CNN] Total coletado: 20\n",
      "2025-10-25 14:20:09 | INFO | noticias | [CNN] Driver encerrado\n",
      "2025-10-25 14:20:09 | INFO | noticias | [CNN] Conclu√≠do em 25.32s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CNN\n",
    "# =========================\n",
    "inicio = time.perf_counter()\n",
    "log.info(\"[CNN] Iniciando extra√ß√£o\")\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    driver = nova_sessao_chrome()\n",
    "    driver.get(fonte_cnn)\n",
    "    time.sleep(5)\n",
    "\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        botao_ok = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".btn-agree button\")))\n",
    "        botao_ok.click()\n",
    "        log.info(\"[CNN] Aceite de cookies clicado\")\n",
    "    except TimeoutException:\n",
    "        log.info(\"[CNN] Bot√£o de cookies n√£o apareceu (ok)\")\n",
    "\n",
    "    total_regs = 0\n",
    "\n",
    "    for pagina in range(2):  # 2 p√°ginas\n",
    "        time.sleep(3)\n",
    "        html_cnn = driver.page_source\n",
    "        soup = BeautifulSoup(html_cnn, \"html.parser\")\n",
    "\n",
    "        container = soup.select_one('ul[data-section=\"article_list\"]')\n",
    "\n",
    "        url_imagem = []\n",
    "        if container:\n",
    "            url_imagem = [img.get(\"src\",\"\") for img in container.select(\"img\") if img.get(\"src\")]\n",
    "\n",
    "        titulos = [t.get_text(strip=True) for t in soup.select(\"div.flex.flex-col.gap-4 h2\")]\n",
    "        links   = [a.get(\"href\",\"\") for a in soup.select(\"div.flex.flex-col.gap-4 a\")]\n",
    "        # Remove link de \"ao vivo\"\n",
    "        links   = [u for u in links if u != 'https://www.cnnbrasil.com.br/ao-vivo/']\n",
    "\n",
    "        data_publicacao = []\n",
    "        for tag in soup.select(\"time\"):\n",
    "            raw = tag.get(\"datetime\")\n",
    "            if raw:\n",
    "                try:\n",
    "                    data_publicacao.append(datetime.fromisoformat(raw).strftime(\"%d/%m/%Y\"))\n",
    "                except Exception:\n",
    "                    data_publicacao.append(\"\")\n",
    "\n",
    "        df_cnn = safe_df_from_lists(\"CNN\", url_imagem, data_publicacao, titulos, links)\n",
    "        total_regs += len(df_cnn)\n",
    "        df_noticias = pd.concat([df_noticias, df_cnn], ignore_index=True)\n",
    "\n",
    "        log.info(f\"[CNN] P√°gina {pagina+1}: {len(df_cnn)} registros\")\n",
    "\n",
    "        # Tenta ir para pr√≥xima p√°gina (se existir)\n",
    "        try:\n",
    "            driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Ir para pr√≥xima p√°gina']\").click()\n",
    "            log.info(\"[CNN] Pr√≥xima p√°gina clicada\")\n",
    "        except Exception:\n",
    "            log.info(\"[CNN] Pr√≥xima p√°gina n√£o dispon√≠vel; encerrando pagina√ß√£o\")\n",
    "            break\n",
    "\n",
    "    log.info(f\"[CNN] Total coletado: {total_regs}\")\n",
    "except Exception as e:\n",
    "    log.exception(f\"[CNN] ERRO na extra√ß√£o: {e}\")\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        log.info(\"[CNN] Driver encerrado\")\n",
    "\n",
    "log.info(f\"[CNN] Conclu√≠do em {time.perf_counter()-inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07e5f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 14:20:31 | INFO | noticias | [Folha] Iniciando extra√ß√£o\n",
      "2025-10-25 14:20:43 | INFO | noticias | [Folha] 'Mostrar mais' clicado (rodada 1)\n",
      "2025-10-25 14:20:45 | INFO | noticias | [Folha] 'Mostrar mais' clicado (rodada 2)\n",
      "2025-10-25 14:20:48 | INFO | noticias | [Folha] 'Mostrar mais' clicado (rodada 3)\n",
      "2025-10-25 14:20:50 | INFO | noticias | [Folha] 'Mostrar mais' clicado (rodada 4)\n",
      "2025-10-25 14:20:53 | WARNING | noticias | [Folha] Listas com tamanhos diferentes: img=99 | data=99 | tit=103 | link=103. Fazendo pad para 103\n",
      "2025-10-25 14:20:53 | INFO | noticias | [Folha] Coletados 99 registros\n",
      "2025-10-25 14:20:55 | INFO | noticias | [Folha] Driver encerrado\n",
      "2025-10-25 14:20:55 | INFO | noticias | [Folha] Conclu√≠do em 24.45s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FOLHA\n",
    "# =========================\n",
    "inicio = time.perf_counter()\n",
    "log.info(\"[Folha] Iniciando extra√ß√£o\")\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    driver = nova_sessao_chrome()\n",
    "    driver.get(fonte_folha)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    for rodada in range(4):\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"main .row, main article, main li\")))\n",
    "            candidatos = driver.find_elements(By.CSS_SELECTOR, \"button.c-button.c-button--expand\")\n",
    "            if candidatos:\n",
    "                botao = candidatos[0]\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", botao)\n",
    "                time.sleep(0.5)\n",
    "                try:\n",
    "                    botao.click()\n",
    "                    log.info(f\"[Folha] 'Mostrar mais' clicado (rodada {rodada+1})\")\n",
    "                except Exception:\n",
    "                    driver.execute_script(\"arguments[0].click();\", botao)\n",
    "                    log.info(f\"[Folha] 'Mostrar mais' clicado via JS (rodada {rodada+1})\")\n",
    "            else:\n",
    "                log.info(\"[Folha] Bot√£o 'Mostrar mais' n√£o encontrado; parando\")\n",
    "                break\n",
    "        except TimeoutException:\n",
    "            log.info(\"[Folha] Timeout aguardando conte√∫do; seguindo\")\n",
    "        finally:\n",
    "            time.sleep(1.5)\n",
    "\n",
    "    html_folha = driver.page_source\n",
    "    soup = BeautifulSoup(html_folha, \"html.parser\")\n",
    "\n",
    "    url_imagem = [img.get(\"data-src\",\"\") for img in soup.select(\"img.c-headline__image\") if img.get(\"data-src\")]\n",
    "    titulos    = [t.get_text(strip=True) for t in soup.select(\"h2.c-headline__title\")]\n",
    "    links      = [a.get(\"href\",\"\") for a in soup.select(\"a.c-headline__url\")]\n",
    "\n",
    "    data_publicacao_raw = [t.get(\"datetime\",\"\") for t in soup.select(\"time.c-headline__dateline[itemprop='datePublished']\")]\n",
    "    data_publicacao = []\n",
    "    for d in data_publicacao_raw:\n",
    "        if d:\n",
    "            try:\n",
    "                data_publicacao.append(datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\").strftime(\"%d/%m/%Y\"))\n",
    "            except Exception:\n",
    "                # tente outro formato comum\n",
    "                try:\n",
    "                    data_publicacao.append(datetime.fromisoformat(d).strftime(\"%d/%m/%Y\"))\n",
    "                except Exception:\n",
    "                    data_publicacao.append(\"\")\n",
    "\n",
    "    df_folha = safe_df_from_lists(\"Folha\", url_imagem, data_publicacao, titulos, links)\n",
    "    # opcional: manter apenas linhas com imagem\n",
    "    df_folha = df_folha[df_folha[\"urlImagem\"] != \"\"]\n",
    "    df_noticias = pd.concat([df_noticias, df_folha], ignore_index=True)\n",
    "\n",
    "    log.info(f\"[Folha] Coletados {len(df_folha)} registros\")\n",
    "except Exception as e:\n",
    "    log.exception(f\"[Folha] ERRO na extra√ß√£o: {e}\")\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        log.info(\"[Folha] Driver encerrado\")\n",
    "\n",
    "log.info(f\"[Folha] Conclu√≠do em {time.perf_counter()-inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53717c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 14:21:07 | INFO | noticias | [ETL] Normalizando e removendo duplicatas\n",
      "2025-10-25 14:21:07 | INFO | noticias | [ETL] Linhas antes: 151 | ap√≥s dedupe: 151 | removidas: 0\n",
      "2025-10-25 14:21:07 | INFO | noticias | [ETL] Amostra:\n",
      "2025-10-25 14:21:07 | INFO | noticias |                                                                                                                                                                                                                                                        urlImagem dataPublicacao                                                                                               titulo                                                                  link fonte        dataExtracao                 hash\n",
      "                         https://s2-g1.glbimg.com/5WrVoTrL2c24Do4OEg3tDprvptE=/540x304/top/smart/https://i.s3.glbimg.com/v1/AUTH_59edd422c0c84a879bd37670ae4f538a/internal_photos/bs/2024/1/P/tCWnbnTHuFIxLcWlmcGA/globo-canal-5-20241101-1800-frame-310355.jpeg     2025-10-24     D√≥lar tem leve alta e fecha a R$ 5,39 com dados de infla√ß√£o no Brasil e nos EUA; Ibovespa avan√ßa https://g1.globo.com/economia/noticia/2025/10/24/dolar-ibovespa.ghtml    G1 2025-10-25 14:19:16 -2458786076684234543\n",
      "                                                                                                                https://s2-g1.glbimg.com/B2YXPBxijNajZUUdb5491y0ehy4=/540x304/top/smart/filters:max_age(3600)/https://s01.video.glbimg.com/deo/vi/31/59/14035931     2025-10-23                       D√≥lar cai e fecha a R$ 5,38 ap√≥s san√ß√£o dos EUA contra a R√∫ssia; Ibovespa sobe https://g1.globo.com/economia/noticia/2025/10/23/dolar-ibovespa.ghtml    G1 2025-10-25 14:19:16  -482048036986572602\n",
      "https://s2-g1.glbimg.com/Bf5ck8uP4djHFeG95zeBzWqBe-Y=/540x304/top/smart/https://i.s3.glbimg.com/v1/AUTH_59edd422c0c84a879bd37670ae4f538a/internal_photos/bs/2025/R/L/9pGXOhQjWMbLFeAbXt6w/2025-01-22t173936z-1-lynxnpel0l0qj-rtroptp-4-fluxo-cambial-semanal.jpg     2025-10-22 D√≥lar tem leve alta e fecha a R$ 5,39 com expectativa por encontro entre Lula e Trump; Ibovespa sobe https://g1.globo.com/economia/noticia/2025/10/22/dolar-ibovespa.ghtml    G1 2025-10-25 14:19:16  3753148881075888665\n",
      "2025-10-25 14:21:07 | INFO | noticias | [ETL] Conclu√≠do em 0.07s\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# NORMALIZA√á√ÉO & DEDUPE\n",
    "# =========================\n",
    "inicio = time.perf_counter()\n",
    "log.info(\"[ETL] Normalizando e removendo duplicatas\")\n",
    "\n",
    "df = df_noticias.copy()\n",
    "\n",
    "# Datas\n",
    "for col in [\"dataPublicacao\", \"dataExtracao\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Hash para dedupe por t√≠tulo+fonte\n",
    "df[\"hash\"] = (df[\"titulo\"].fillna(\"\") + \"|\" + df[\"fonte\"].fillna(\"\")).apply(lambda x: str(hash(x)))\n",
    "antes = len(df)\n",
    "df = df.drop_duplicates(subset=[\"hash\"])\n",
    "apos = len(df)\n",
    "\n",
    "log.info(f\"[ETL] Linhas antes: {antes} | ap√≥s dedupe: {apos} | removidas: {antes-apos}\")\n",
    "log.info(\"[ETL] Amostra:\")\n",
    "log.info(df.head(3).to_string(index=False))\n",
    "\n",
    "log.info(f\"[ETL] Conclu√≠do em {time.perf_counter()-inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6baf6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 14:21:19 | INFO | noticias | [DB] Iniciando persist√™ncia em DuckDB\n",
      "2025-10-25 14:21:19 | INFO | noticias | [DB] Tabela verificada/criada\n",
      "2025-10-25 14:21:19 | INFO | noticias | [DB] Inseridos -1 registros\n",
      "2025-10-25 14:21:19 | INFO | noticias | [DB] Export gerado em ./exports/noticias.csv\n",
      "2025-10-25 14:21:19 | INFO | noticias | [DB] Conex√£o encerrada\n",
      "2025-10-25 14:21:20 | INFO | noticias | [DB] Conclu√≠do em 0.36s\n",
      "2025-10-25 14:21:20 | INFO | noticias | ===== FIM EXECU√á√ÉO noticias.ipynb =====\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DUCKDB\n",
    "# =========================\n",
    "inicio = time.perf_counter()\n",
    "log.info(\"[DB] Iniciando persist√™ncia em DuckDB\")\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "DB_PATH = \"dados_dolar.duckdb\"\n",
    "TABLE   = \"noticias\"\n",
    "\n",
    "os.makedirs(\"exports\", exist_ok=True)\n",
    "\n",
    "con = None\n",
    "try:\n",
    "    con = duckdb.connect(DB_PATH)\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "            urlImagem       TEXT,\n",
    "            dataPublicacao  TIMESTAMP,\n",
    "            titulo          TEXT,\n",
    "            link            TEXT,\n",
    "            fonte           TEXT,\n",
    "            dataExtracao    TIMESTAMP,\n",
    "            hash            TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "    log.info(\"[DB] Tabela verificada/criada\")\n",
    "\n",
    "    # registra DF e insere\n",
    "    con.register(\"df_stage\", df)\n",
    "    inseridos = con.execute(f\"\"\"\n",
    "        INSERT INTO {TABLE}\n",
    "        SELECT urlImagem, dataPublicacao, titulo, link, fonte, dataExtracao, hash\n",
    "        FROM df_stage;\n",
    "    \"\"\").rowcount\n",
    "    log.info(f\"[DB] Inseridos {inseridos if inseridos is not None else 0} registros\")\n",
    "\n",
    "    # export CSV a partir do DF (ou da tabela, se preferir)\n",
    "    df.to_csv(\"./exports/noticias.csv\", index=False, encoding=\"utf-8\")\n",
    "    log.info(\"[DB] Export gerado em ./exports/noticias.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.exception(f\"[DB] ERRO na persist√™ncia: {e}\")\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()\n",
    "        log.info(\"[DB] Conex√£o encerrada\")\n",
    "\n",
    "log.info(f\"[DB] Conclu√≠do em {time.perf_counter()-inicio:.2f}s\")\n",
    "log.info(\"===== FIM EXECU√á√ÉO noticias.ipynb =====\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
